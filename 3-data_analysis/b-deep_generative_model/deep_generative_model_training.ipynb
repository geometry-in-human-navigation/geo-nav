{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning latent representations of geometry \n",
    "\n",
    "A deep generative model (a variational autoencoder) is used to to infer scene depth structure, to identify the brain regions that carry neural signals of geometry representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conda  \n",
    "- conda create -n deeplearning python=3.9\n",
    "- conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "- conda install numpy scipy matplotlib pandas\n",
    "- conda install -c conda-forge visdom\n",
    "- conda install -c anaconda scikit-image\n",
    "- conda install -c conda-forge vit-pytorch\n",
    "\n",
    "- https://github.com/lucidrains/vit-pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start visdom server\n",
    "- python -m visdom.server -port 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Transformer Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import PIL.Image\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import model_vitae as ViTAE\n",
    "from visdom import Visdom\n",
    "import utils\n",
    "from scipy.io import savemat\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_type = \"monotonic_annealing\" # monotonic_annealing, cyclical_annealing, betavae_basic, betavae_capacity_control\n",
    "\n",
    "# hyperparameters\n",
    "EPOCH = 6\n",
    "HIDDEN_DIM = 8 #8, 16, 32, 96\n",
    "BETA = 1.0\n",
    "BATCH_SIZE = 96\n",
    "CAPACITY_MAX = 2000.0 #2000.0\n",
    "ORTH_FACTOR = 500.0\n",
    "LATENT_UNIT_RANGE = 2.5\n",
    "\n",
    "# Plots\n",
    "viz = Visdom(port=4000)\n",
    "loss_plt = utils.VisdomPlotter(viz=viz, env_name='main')\n",
    "\n",
    "# dataset\n",
    "root_dir = \"/media/statespace/Spatial/sptialworkspace/spatialfMRI/simulation/carla_ws/recording/output_random_pose\"\n",
    "\n",
    "run_sample_size = 8000\n",
    "run_num = 7\n",
    "town_num = 8\n",
    "sample_step = 1\n",
    "data_sampler = utils.dataloader_rgb_depth(train_or_test=\"train\", root_dir=root_dir, \\\n",
    "    start_index=10, run_num=run_num, town_num=town_num, run_sample_size=run_sample_size, sample_step=sample_step, \\\n",
    "    batch_size=BATCH_SIZE, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = int(run_num*run_sample_size/sample_step)\n",
    "batch_size = 32\n",
    "\n",
    "batch_num = num_images//batch_size\n",
    "\n",
    "for batch_index in range(batch_num):\n",
    "\n",
    "    batch_rgb_images, batch_depth_images = data_sampler.images_sample(batch_num=batch_index, batch_size=batch_size)\n",
    "    batch_str = '{batch_index:03}/{batch_num:03}'\n",
    "    if batch_index%20==0:\n",
    "        print(batch_str.format(batch_index=batch_index, batch_num=batch_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image string:\", glob.glob(data_sampler.current_wildcard_rgb_name)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"batch_rgb_images shape:\", batch_rgb_images.shape)\n",
    "data = batch_rgb_images[0,:,:,:]\n",
    "plt.imshow(data[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(\"batch_depth_images shape:\", batch_depth_images.shape)\n",
    "data = batch_depth_images[0,:,:]\n",
    "plt.imshow(data)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTAE.ViTAE_RGB2Depth(\n",
    "    image_size = (320, 640),\n",
    "    beta=BETA,\n",
    "    orth_factor = ORTH_FACTOR,\n",
    "    latent_unit_range = LATENT_UNIT_RANGE,\n",
    "    patch_size = 32,\n",
    "    dim_latent = HIDDEN_DIM,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    ").to(device)\n",
    "\n",
    "load_model_params = True\n",
    "if load_model_params == True:\n",
    "    model.load_state_dict(torch.load('results/vitea_params_losstype-monotonic_annealing_hdim-8_beta-1.0_EPOCH-6_batch_size-96_capacity_max-2000.0_epoch-6_Sep_1.pkl'))\n",
    "\n",
    "# Setting the optimiser\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "current_time = datetime.now()\n",
    "print('Starting model evaluation...')\n",
    "\n",
    "model.eval()\n",
    "loss = []\n",
    "recon_loss = []\n",
    "kld_loss = []\n",
    "total_kld = []\n",
    "\n",
    "for epoch in np.arange(0, 1):\n",
    "    for batch_index in range(4200,4201):\n",
    "        \n",
    "        batch_rgb_images, batch_depth_images = data_sampler.images_sample(batch_index=batch_index)\n",
    "        # batch_rgb_images = batch_rgb_images.to(device)\n",
    "        # batch_depth_images = batch_depth_images.to(device)\n",
    "\n",
    "        # forward\n",
    "        if loss_type == \"monotonic_annealing\":\n",
    "            beta_vae_loss = model.calc_monotonic_annealing_loss(\n",
    "                    rgb_images=batch_rgb_images, \n",
    "                    depth_images=batch_depth_images,\n",
    "                    capacity_num_iter=EPOCH*data_sampler.batch_num,\n",
    "                    capacity_stop_iter=epoch*data_sampler.batch_num + batch_index)\n",
    "        elif loss_type == \"cyclical_annealing\":\n",
    "            beta_vae_loss = model.calc_cyclical_annealing_loss(\n",
    "                    rgb_images=batch_rgb_images, \n",
    "                    depth_images=batch_depth_images,\n",
    "                    num_cycles=5,\n",
    "                    capacity_num_iter=EPOCH*data_sampler.batch_num,\n",
    "                    capacity_stop_iter=epoch*data_sampler.batch_num + batch_index)\n",
    "        elif loss_type == \"betavae_basic\":\n",
    "            beta_vae_loss = model.calc_beta_vae_basic_loss(rgb_images=batch_rgb_images, depth_images=batch_depth_images)\n",
    "        elif loss_type == \"betavae_capacity_control\":\n",
    "            beta_vae_loss = model.calc_beta_vae_capacity_control_loss(\n",
    "                    rgb_images=batch_rgb_images, \n",
    "                    depth_images=batch_depth_images,\n",
    "                    capacity_max=CAPACITY_MAX,\n",
    "                    capacity_num_iter=EPOCH*data_sampler.batch_num,\n",
    "                    capacity_stop_iter=epoch*data_sampler.batch_num + batch_index\n",
    "                )\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        beta_vae_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show loss\n",
    "        loss.append(beta_vae_loss.detach().cpu().numpy()[0])\n",
    "        recon_loss.append(model.recon_loss.detach().cpu().numpy())\n",
    "        kld_loss.append(model.kld_loss.detach().cpu().numpy()[0])\n",
    "        total_kld.append(model.total_kld.detach().cpu().numpy()[0])\n",
    "\n",
    "        # if batch_index%5==0:\n",
    "        #     batch_str = '{batch_index:03}/{batch_num:03}'\n",
    "        #     print(batch_str.format(batch_index=batch_index, batch_num=batch_num))\n",
    "\n",
    "        # loss_plt.plot(x=np.arange(0,len(loss)), y=loss, var_name=\"loss\", split_name=\"loss\", title_name=\"loss along time\")\n",
    "\n",
    "    ##================================epoch================================\n",
    "    # print(\"epoch:\", epoch + 1)\n",
    "    # if (epoch + 1) % (1) == 0:\n",
    "        # loss_plt.plot(x=np.arange(0,len(loss)), y=loss, var_name='loss', split_name=['loss', 'recon_loss', 'kld_loss'], title_name=\"loss along time\")\n",
    "        loss_stack = np.column_stack((np.array(loss),np.array(recon_loss),np.array(kld_loss),np.array(total_kld)))\n",
    "        # loss_plt.multiplot(x=np.arange(0,len(loss)), y=loss_stack, var_name='multiloss')\n",
    "        \n",
    "        rgb_images_viz = make_grid(batch_rgb_images[0:9].detach(), normalize=True, nrow=3)\n",
    "        depth_images_viz = make_grid(torch.unsqueeze(batch_depth_images[0:9], 1).detach(), normalize=True, nrow=3)\n",
    "        recon_images_viz = make_grid(torch.unsqueeze(model.depth_recon[0:9], 1).detach(), normalize=True, nrow=3)\n",
    "        images = torch.stack([rgb_images_viz, depth_images_viz, recon_images_viz], dim=0).cpu()\n",
    "        # loss_plt.rgb_depth_images(images=images, var_name=\"depth\", split_name=\"depth\", title_name=\"rbg recon depth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visulize rgb, depth and recon depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_index = 10\n",
    "# batch_rgb_images, batch_depth_images = data_sampler.images_sample(batch_num=batch_index, batch_size=batch_size)\n",
    "# batch_rgb_images = batch_rgb_images.to(device)\n",
    "# batch_depth_images = batch_depth_images.to(device)\n",
    "\n",
    "rgb_images_viz = make_grid(batch_rgb_images[0:9].detach(), normalize=True, nrow=3)\n",
    "depth_images_viz = make_grid(torch.unsqueeze(batch_depth_images[0:9], 1).detach(), normalize=True, nrow=3)\n",
    "recon_images_viz = make_grid(torch.unsqueeze(model.depth_recon[0:9], 1).detach(), normalize=True, nrow=3)\n",
    "images = torch.stack([rgb_images_viz, depth_images_viz, recon_images_viz], dim=0).cpu()\n",
    "\n",
    "images_grids = make_grid(images, normalize=True, nrow=3)\n",
    "print(images_grids.shape)\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.imshow(np.transpose(images_grids,(1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_percentage_error \n",
    "y_true = batch_depth_images.cpu().numpy()\n",
    "y_pred = torch.squeeze(model.depth_recon).detach().cpu().numpy()\n",
    "\n",
    "print(\"y_true.shape:\",y_true.shape)\n",
    "print(\"y_pred.shape:\",y_pred.shape)\n",
    "\n",
    "print(\"mean_squared_error:\",mean_squared_error(y_true.reshape(BATCH_SIZE,-1), y_pred.reshape(BATCH_SIZE,-1)))\n",
    "print(\"explained_variance_score:\",explained_variance_score(y_true.reshape(BATCH_SIZE,-1), y_pred.reshape(BATCH_SIZE,-1)))\n",
    "plt.plot(model.mu.detach().cpu().numpy().flatten())\n",
    "plt.show()\n",
    "# plt.plot(model.logvar.detach().cpu().numpy().flatten())\n",
    "# plt.show()\n",
    "plt.plot(y_true.flatten())\n",
    "plt.show()\n",
    "plt.plot(y_pred.flatten())\n",
    "plt.show()\n",
    "# print((y_true.flatten(), y_pred.flatten()))\n",
    "\n",
    "temp_coef = np.corrcoef(y_true.reshape(BATCH_SIZE,-1) - y_true.reshape(BATCH_SIZE,-1).mean(0), \\\n",
    "  y_pred.reshape(BATCH_SIZE,-1) - y_pred.reshape(BATCH_SIZE,-1).mean(0))\n",
    "plt.imshow(temp_coef)\n",
    "plt.colorbar()\n",
    "# print(temp_coef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traverse hiddent units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "limit = 3.0\n",
    "# inter = limit*2.0/10.0\n",
    "decoder = model.decoder\n",
    "encoder = model.encoder\n",
    "num_image_grid = 5\n",
    "interpolation = np.linspace(-limit, limit, num=num_image_grid)\n",
    "\n",
    "# for batch_index in range(batch_num):\n",
    "batch_index = 3500 # 3500, 90 for first night index\n",
    "# batch_rgb_images, batch_depth_images = data_sampler.images_sample(batch_index=batch_index)\n",
    "# batch_rgb_images = batch_rgb_images.to(device)\n",
    "# batch_depth_images = batch_depth_images.to(device)\n",
    "\n",
    "fixed_idx = 30\n",
    "fixed_img = batch_rgb_images[fixed_idx]\n",
    "fixed_img = torch.unsqueeze(fixed_img, axis=0).to(device=device)\n",
    "fixed_img_z = (torch.sigmoid(model.encoder(fixed_img)[:, :model.dim_latent])*2.0 - 1.0)*model.latent_unit_range\n",
    "fixed_depth_recon = decoder(fixed_img_z)\n",
    "\n",
    "z_ori = fixed_img_z\n",
    "z_ori = torch.zeros_like(fixed_img_z)\n",
    "# print(\"z_ori:\", z_ori.shape , z_ori.detach().cpu().numpy())\n",
    "\n",
    "samples = []\n",
    "for row in range(model.dim_latent):\n",
    "    z = z_ori.clone()\n",
    "    for val in interpolation:\n",
    "        z[:, row] = val\n",
    "        sample = decoder(z)\n",
    "        sample = torch.unsqueeze(sample,axis=0)\n",
    "        samples.append(sample)\n",
    "        val_str = \"{0:.3f}\".format(val)\n",
    "        # print(\"row:\",row,\",val:\",val_str,\",\",z.detach().cpu().numpy()[0,0:5])\n",
    "\n",
    "samples = torch.cat(samples, dim=0).cpu()\n",
    "\n",
    "samples_grids = make_grid(samples, normalize=False, nrow=num_image_grid)\n",
    "print(samples_grids.shape)\n",
    "# print('max:',np.max(samples_grids.numpy().flatten()))\n",
    "# print('min:',np.min(samples_grids.numpy().flatten()))\n",
    "\n",
    "org_rgb_image = batch_rgb_images[fixed_idx].cpu().numpy()\n",
    "org_depth_image = batch_depth_images[fixed_idx].cpu().numpy()\n",
    "\n",
    "temp_orb_rgb_image = (org_rgb_image - np.min(org_rgb_image.flatten())) \\\n",
    "    /(np.max(org_rgb_image.flatten()) - np.min(org_rgb_image.flatten()))\n",
    "f, axarr = plt.subplots(1, 3, figsize=(30,10))\n",
    "axarr[0].imshow(np.transpose(temp_orb_rgb_image,(1, 2, 0)))\n",
    "axarr[1].imshow(org_depth_image, cmap='jet', vmax=1, vmin=0)\n",
    "axarr[2].imshow(np.squeeze(fixed_depth_recon.detach().cpu().numpy()), cmap='jet', vmax=1, vmin=0)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(30,90))\n",
    "# plt.imshow(np.transpose(samples_grids,(1, 2, 0)))\n",
    "# plt.imshow(samples_grids[0], cmap='jet', vmax=1, vmin=0)\n",
    "plt.imshow(samples_grids[0], cmap='jet')\n",
    "# plt.imshow(np.squeeze(sample.detach().cpu().numpy()), cmap='jet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mu = model.mu.detach().cpu()\n",
    "print('temp_mu shape:', temp_mu.shape)\n",
    "\n",
    "# corr_mu = torch.corrcoef(temp_mu.T)\n",
    "\n",
    "triu_corr_mu = torch.triu(torch.corrcoef(temp_mu.T) + 1.0, diagonal=1) \n",
    "# triu_loss = torch.sum(torch.flatten(triu_corr_mu))\n",
    "triu_loss = torch.flatten(triu_corr_mu).sum(0,True)\n",
    "print(triu_loss)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(triu_corr_mu.numpy() - 1.0, interpolation='None', cmap='jet')\n",
    "plt.title('corr_mu')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test images traverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "limit = 3.0\n",
    "# inter = limit*2.0/10.0\n",
    "decoder = model.decoder\n",
    "encoder = model.encoder\n",
    "interpolation = np.linspace(-limit, limit, num=10)\n",
    "\n",
    "# town07 batch: 3500 -> 4080\n",
    "for batch_index in np.arange(3500, 4080, 25):\n",
    "    # batch_index = 3500\n",
    "    batch_rgb_images, batch_depth_images = data_sampler.images_sample(batch_index=batch_index)\n",
    "    batch_rgb_images = batch_rgb_images.to(device)\n",
    "    batch_depth_images = batch_depth_images.to(device)\n",
    "\n",
    "    fixed_idx = 0\n",
    "    fixed_img = batch_rgb_images[fixed_idx]\n",
    "    fixed_img = torch.unsqueeze(fixed_img, axis=0).to(device=device)\n",
    "    fixed_img_z = (torch.sigmoid(model.encoder(fixed_img)[:, :model.dim_latent])*2.0 - 1.0)*model.latent_unit_range\n",
    "    fixed_depth_recon = decoder(fixed_img_z)\n",
    "\n",
    "    z_ori = fixed_img_z\n",
    "    z_ori = torch.zeros_like(fixed_img_z)\n",
    "    # print(\"z_ori:\", z_ori.shape , z_ori.detach().cpu().numpy())\n",
    "\n",
    "    samples = []\n",
    "    for row in range(model.dim_latent):\n",
    "        z = z_ori.clone()\n",
    "        for val in interpolation:\n",
    "            z[:, row] = val\n",
    "            sample = decoder(z)\n",
    "            sample = torch.unsqueeze(sample,axis=0)\n",
    "            samples.append(sample)\n",
    "            val_str = \"{0:.3f}\".format(val)\n",
    "            # print(\"row:\",row,\",val:\",val_str,\",\",z.detach().cpu().numpy()[0,0:5])\n",
    "\n",
    "    samples = torch.cat(samples, dim=0).cpu()\n",
    "\n",
    "    samples_grids = make_grid(samples, normalize=False, nrow=10)\n",
    "    # print(samples_grids.shape)\n",
    "    # print('max:',np.max(samples_grids.numpy().flatten()))\n",
    "    # print('min:',np.min(samples_grids.numpy().flatten()))\n",
    "\n",
    "    org_rgb_image = batch_rgb_images[fixed_idx].cpu().numpy()\n",
    "    org_depth_image = batch_depth_images[fixed_idx].cpu().numpy()\n",
    "\n",
    "    temp_orb_rgb_image = (org_rgb_image - np.min(org_rgb_image.flatten())) \\\n",
    "        /(np.max(org_rgb_image.flatten()) - np.min(org_rgb_image.flatten()))\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(10,10))\n",
    "    axarr[0].imshow(np.transpose(temp_orb_rgb_image,(1, 2, 0)))\n",
    "    axarr[1].imshow(org_depth_image, cmap='jet', vmax=1, vmin=0)\n",
    "    axarr[2].imshow(np.squeeze(fixed_depth_recon.detach().cpu().numpy()), cmap='jet', vmax=1, vmin=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "536b69f95244d1f49a4e722912c058d25ae6a826de010f5e4fe2961a82882293"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
